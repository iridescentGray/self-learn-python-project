{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建连接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdfs\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_client = hdfs.InsecureClient(\"http://localhost:9870\",user=\"hadoop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录/列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取目录操作权限\n",
    "默认是没有权限操作目录的，需在hadoop shell中执行如下命令\n",
    "hadoop fs  -chmod 777 /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "{'accessTime': 0,\n",
      " 'blockSize': 0,\n",
      " 'childrenNum': 0,\n",
      " 'fileId': 16385,\n",
      " 'group': 'supergroup',\n",
      " 'length': 0,\n",
      " 'modificationTime': 1701683192308,\n",
      " 'owner': 'hadoop',\n",
      " 'pathSuffix': '',\n",
      " 'permission': '755',\n",
      " 'replication': 0,\n",
      " 'snapshotEnabled': True,\n",
      " 'storagePolicy': 0,\n",
      " 'type': 'DIRECTORY'}\n"
     ]
    }
   ],
   "source": [
    "print(hdfs_client.list(\"/\"))\n",
    "pprint(hdfs_client.list(\"/\", status=True))\n",
    "pprint(hdfs_client.status(\"/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n",
      "['b']\n",
      "['c']\n"
     ]
    }
   ],
   "source": [
    "hdfs_client.makedirs(\"/a/b/c\", permission=777)\n",
    "print(hdfs_client.list(\"/\")) \n",
    "print(hdfs_client.list(\"/a\")) \n",
    "print(hdfs_client.list(\"/a/b\"))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看目录详情\n",
    "查看当前目录下有多少个子目录、多少文件等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'directoryCount': 4, 'ecPolicy': '', 'fileCount': 0, 'length': 0, 'quota': 9223372036854775807, 'snapshotDirectoryCount': 0, 'snapshotFileCount': 0, 'snapshotLength': 0, 'snapshotSpaceConsumed': 0, 'spaceConsumed': 0, 'spaceQuota': -1, 'typeQuota': {}}\n"
     ]
    }
   ],
   "source": [
    "print(hdfs_client.content(\"/\", strict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遍历目录\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/', ['a'], [])\n",
      "('/a', ['b'], [])\n",
      "('/a/b', ['c'], [])\n",
      "('/a/b/c', [], [])\n"
     ]
    }
   ],
   "source": [
    "for file in hdfs_client.walk(\"/\"):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读/写/删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 普通写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attiontion\n",
    "\n",
    "* 需要有通往namenode/datanode的网络(需要配置hosts文件,解析datanode的域名到127.0.0.1)\n",
    "如果没有网络，会报错 ConnectionError: HTTPConnectionPool(host='e149fa31e917', port=9864): Max retries exceeded with url\n",
    "* 其实从这里也能看出来,因为namenode仅仅负责路由，只会把datanode的地址返回给我们，让我们自己继续调用，所以必须有datanode的权限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'test.json']\n",
      "b'{\"key\": \"value\"}'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "write_data=json.dumps({\n",
    "    \"key\":\"value\"\n",
    "})\n",
    "with hdfs_client.write('/test.json') as writer:\n",
    "    writer.write(write_data)\n",
    "    \n",
    "print(hdfs_client.list(\"/\")) \n",
    "\n",
    "with hdfs_client.read('/test.json') as reader:\n",
    "    features = reader.read()\n",
    "    print(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"key\": \"value\"}Winter is coming againWinter is coming again\n"
     ]
    }
   ],
   "source": [
    "with hdfs_client.write(\"/test.json\", append=True) as writer:\n",
    "    writer.write(bytes(\"Winter is coming again\", encoding=\"utf-8\"))\n",
    "    writer.write(bytes(\"Winter is coming again\", encoding=\"utf-8\"))\n",
    "\n",
    "with hdfs_client.read(\"/test.json\") as reader:\n",
    "    print(str(reader.read(), encoding=\"utf-8\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winter is coming again\n"
     ]
    }
   ],
   "source": [
    "with hdfs_client.write(\"/test.json\", overwrite=True) as writer:\n",
    "    writer.write(bytes(\"Winter is coming again\", encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "with hdfs_client.read(\"/test.json\") as reader:\n",
    "    print(str(reader.read(), encoding=\"utf-8\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    hdfs_client.delete(\"/test.json\")\n",
    "except Exception as e:\n",
    "    print(e) \n",
    "print(hdfs_client.list(\"/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上传/下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "hdfs_client.upload(hdfs_path=\"/\", local_path=\"./test.json\")\n",
    "print(\"test.json\" in hdfs_client.list(\"/\"))  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"upload\":\"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "hdfs_client.download(hdfs_path=\"/test.json\", local_path=\"./remote_json\")\n",
    "print(open(\"./remote_json\", \"r\", encoding=\"utf-8\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件权限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置所有者\n",
    "# hdfs_client.set_owner\n",
    "#设置权限\n",
    "# hdfs_client.set_permission\n",
    "#设置副本系数\n",
    "# hdfs_client.set_replication\n",
    "#设置时间\n",
    "# hdfs_client.set_times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
