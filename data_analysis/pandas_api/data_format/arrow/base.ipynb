{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas支持\n",
    "\n",
    "* pandas 在 2.0 的时候，可以采用 pyarrow 作为后端。\n",
    "    1.  之前的版本Pandas 的数据在内存中基本都是以 Numpy 数组的形式存在\n",
    "    2.  Numpy 本身并不是为 DataFrame 而设计，对于一些数据类型的支持并不是很好。例如 NaN，NaT，pd.NA\n",
    "* Arrow 是更契合的内存数据结构，不仅速度更快，也更省内存，对数据类型的支持也更好\n",
    "* https://pandas.pydata.org/docs/user_guide/pyarrow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = pyarrow.array([1, 2, 3])\n",
    "print(arr1.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = pyarrow.array([1, 2, 3], type=pyarrow.int32())\n",
    "print(arr2.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 在存储数据时默认使用 Numpy Array\n",
    "pd.Series([1, 2, 3], dtype=\"int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定 Pyarrow，表示使用 Arrow 格式来存储数据\n",
    "pd.Series([1, 2, 3], dtype=\"int64[pyarrow]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置默认的IO引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/docs/user_guide/options.html\n",
    "pd.options.io.parquet.engine = \"pyarrow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础读写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({\n",
    "    \"name\": [\"satori\", \"koishi\", \"marisa\", \"cirno\"],\n",
    "    \"age\": [17, 16, 18, 40],\n",
    "    \"gender\": [\"female\"] * 4\n",
    "})\n",
    "\n",
    "df.to_parquet(\n",
    "    \"base.parquet.gz\",\n",
    "    # 需要 pip install pyarrow\n",
    "    engine=\"pyarrow\",\n",
    "    # 压缩方式，可选择：'snappy'(默认), 'gzip', 'brotli', None\n",
    "    compression=\"gzip\",\n",
    "    # 是否把 DataFrame 自带的索引写入，True\n",
    "    # 但要注意的是，索引会以 range 对象的形式写入到元数据中,不会占用太多空间\n",
    "    # 因此，并且速度还更快\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"base.parquet.gz\",\n",
    "                     engine=\"pyarrow\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分区\n",
    "\n",
    "* girl.parquet.gz 不再是文件，而是一个目录，然后目录里面会出现 4 个子目录。\n",
    "* 因为我们是按照 name 分区的，而 name 有 4 个不同的值\n",
    "* 只有那些专门用于分类、元素重复率非常高的字段，才适合做分区字段，最典型的就是日期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"name\": [\"satori\", \"koishi\", \"marisa\", \"cirno\"] * 2,\n",
    "    \"age\": [17, 16, 18, 40] * 2,\n",
    "    \"gender\": [\"female\"] * 8\n",
    "})\n",
    "\n",
    "# \n",
    "df.to_parquet(\n",
    "    \"partition.parquet.gz\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"gzip\",\n",
    "    # 按照 \"name\" 字段分区\n",
    "    partition_cols=[\"name\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"partition.parquet.gz\",\n",
    "                     engine=\"pyarrow\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 部分读取\n",
    "\n",
    "读取分区后的部分记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取name=satori的记录\n",
    "pd.read_parquet(\"partition.parquet.gz/name=satori\",\n",
    "                     engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = pd.DataFrame({\n",
    "    \"p1\": [\"a\"] * 4 + [\"b\"] * 4 + [\"c\"] * 4,\n",
    "    \"p2\": [\"X\", \"X\", \"Y\", \"Y\"] * 3,\n",
    "    \"p3\": np.random.randint(1, 100, size=(12,))\n",
    "})\n",
    "\n",
    "df.to_parquet(\n",
    "    \"many_partition.parquet.gz\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"gzip\",\n",
    "    # 按照 \"p1\" 和 \"p2\" 字段分区\n",
    "    partition_cols=[\"p1\", \"p2\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取多分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"many_partition.parquet.gz/p1=b\",\n",
    "                     engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"many_partition.parquet.gz/p1=b/p2=X\",\n",
    "                     engine=\"pyarrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
